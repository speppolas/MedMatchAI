# Configurazione LLM con llama.cpp (esclusivo)
LLAMA_CPP_PATH=/home/runner/workspace/llama.cpp/build/bin/main
LLM_MODEL_PATH=/home/runner/workspace/llama.cpp/models/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf

# Optimize performance
LLM_CONTEXT_SIZE=2048
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=256

# Configurazione Modello LLM
LLM_CONTEXT_SIZE=8192               # Dimensione contesto (adatta in base alla memoria)
LLM_TEMPERATURE=0.1                 # Temperatura per controllo casualit√†
LLM_MAX_TOKENS=512                  # Numero massimo di token generati
LLM_TIMEOUT = 10                    # Timeout per l'esecuzione (in secondi)
LLM_EXTRA_PARAMS=                  

# Configurazione Flask (produzione)
FLASK_SECRET_KEY=una_chiave_segreta_lunga_e_casuale
FLASK_ENV=development

# Configurazione Database (PostgreSQL)
SQLALCHEMY_DATABASE_URI=postgresql://medmatchuser:password_sicura@localhost/medmatchint
UPLOAD_FOLDER=uploads

# Configurazione CUDA for llama.cpp (NEW)
GGML_CUDA=yes
GGML_CUDA_FORCE_MMQ=yes
GGML_CUDA_FORCE_CUBLAS=yes

# CUDA Library Path (Add this to ensure CUDA 12.8 is detected)
CUDA_HOME=/usr/local/cuda-12.8
PATH=$CUDA_HOME/bin:$PATH
LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH